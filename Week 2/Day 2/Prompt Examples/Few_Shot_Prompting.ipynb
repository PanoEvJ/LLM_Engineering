{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot\n",
        "\n",
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)\n",
        "\n",
        "Let's explore the world of Few-Shot prompting and all it can do!"
      ],
      "metadata": {
        "id": "UIuhLOcmCdyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai cohere tiktoken -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2y8NcRnD8aa",
        "outputId": "0c8f44f5-3568-4f06-bc72-b2a06a392692"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.2/220.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI Key"
      ],
      "metadata": {
        "id": "jwzSC6pJpORd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tpnsDCfEbsqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "293f59a0-809c-4880-dcf4-71149b7aa254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "# set the OPENAI_API_KEY environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "rQw5oRsAJKcm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Helper Functions\n",
        "\n",
        "These are only meant to display the outputs in a more palatable style - you can call the API directly using the functions as a guide."
      ],
      "metadata": {
        "id": "Btqi64IPpQHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's focus on extending this a bit, and incorporate a `system` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The system message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the system prompt."
      ],
      "metadata": {
        "id": "UPs3ScS1WpoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QSQMFfWKbsqT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(messages: list) -> str:\n",
        "  client = OpenAI()\n",
        "  response = client.chat.completions.create(\n",
        "      messages=messages,\n",
        "      model=\"gpt-4-1106-preview\",\n",
        "  )\n",
        "  del client\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "def wrap_prompt(message: str, role: str) -> dict:\n",
        "    return {\"role\": role, \"content\": message}\n",
        "\n",
        "def m_print(message: str) -> str:\n",
        "    display(Markdown(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if our model can define nonsense terms for us."
      ],
      "metadata": {
        "id": "3zg7PNgKTyni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7aEd_p1sbsqT",
        "outputId": "4ec1b7a7-0867-4015-9daf-d1d1d545633d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "As of my last knowledge update in April 2023, the term \"flimple-dimple\" doesn't appear to be a widely recognized or standard term in English. It seems to be a nonsensical or made-up term, perhaps used playfully or within a specific context that isn't broadly known. It's possible that it could be a term from a children's book, a game, or an inside joke within a certain group or family, but without additional context, it's difficult to provide a precise definition.\n\nIf \"flimple-dimple\" has gained a specific meaning in certain circles or has been coined in a particular piece of media after April 2023, I wouldn't have information on it. It could also be spelled or meant differently, and I might be able to help if you can provide more context or check if there's a typo or alternate spelling."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = wrap_prompt(\"\"\"What is a flimple-dimple?\"\"\", \"user\")\n",
        "\n",
        "m_print(get_response([prompt]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if adding a system prompt helps us."
      ],
      "metadata": {
        "id": "WcC5SCDoT8up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    wrap_prompt(\"You are a creative children's author.\", \"system\"),\n",
        "    wrap_prompt(\"What is a flimple-dimple?\", \"user\")\n",
        "]\n",
        "\n",
        "m_print(get_response(list_of_prompts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "d46b7ee5-46f0-4abf-b87f-de17b11ff7d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A Flimple-Dimple is a curious creature of my imagination, crafted specially to enthrall and delight young readers. Let me spin you a little tale about it:\n\nDeep in the heart of the Whimsy Woods where the Gigglegush River winds, and the Tickletree fruits hang ripe for the picking, lives the elusive Flimple-Dimple. This creature is quite the sight to behold, with its tufty fur of a thousand colors—each shade representing a different mood. During sunrise, its fur sparkles with hues of gold and pink, which is when the Flimple-Dimple feels happiest.\n\nNow, don't let its name fool you, for there is nothing simple about a Flimple-Dimple. It has the ears of a giant rabbit, perfect for eavesdropping on the forest's secrets, and the gentle eyes of an old soul that seem to twinkle with the wisdom of the ages. Its long, squiggly tail helps it balance as it bounces from cloud puff to cloud puff, and its webbed feet are splendid for splashing in the Gigglegush River.\n\nThe diet of a Flimple-Dimple is most peculiar. It feasts on giggles and guffaws, which it harvests from the laughter of children playing in the woods. But fear not; taking your laughter doesn't leave you without—it simply creates more, bubbling up from within until you're swept up in a never-ending cycle of joy.\n\nAt night, when the stars in Whimsy Woods are abuzz with shimmering fireflies, you might just hear the soft hum of a Flimple-Dimple singing to the moon. Its voice has the magic to lull even the most restless of creatures into a restful slumber filled with dreams of adventures yet to come.\n\nThough Flimple-Dimples are shy, if you're ever lucky enough to encounter one, remember to greet it with a smile and a hearty chuckle. In return, it might just grant you a small, glittering feather as a keepsake—a reminder that magic and wonder are never too far away when you've got a Flimple-Dimple in your heart."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is really quite excellent - but isn't necessarily correct.\n",
        "\n",
        "Let's try modifying our system prompt to see if anything changes."
      ],
      "metadata": {
        "id": "MHu56wTnUOQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts[0] = wrap_prompt(\"You are a creative poet.\", \"system\")\n",
        "\n",
        "m_print(get_response(list_of_prompts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "f3f5aef2-2399-4e92-964d-89e5964be4b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In a land where fancy meets whim,\nLies a creature not grim, but slim,\nWith a bounce in its step and a smile in its dimple,\nIt's the curious, capricious flimple-dimple.\n\nWith fur so soft and a tail that's ample,\nIt hops along, leaves a sparkle so simple,\nIts cheeks full of joy, its laughter a jingle,\nThe merry-hearted, ever-darting flimple-dimple.\n\nIn forests of dreams where thoughts intermingle,\nIt plays hide and seek, making introverts giggle,\nWith a twirl and a flip, it's never a bore,\nThis flimple-dimple, a myth, an ever-whimsical lore.\n\nIt teaches us all to take life a bit nimble,\nTo cherish the sweet and embrace the wrinkle,\nSo whenever you're down or caught in life's thimble,\nJust conjure the cheer of the flimple-dimple."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this is a cool and complete answer - what if we wanted to teach the model what a flimple-dimple is?"
      ],
      "metadata": {
        "id": "Ci7pAEGkO_gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ],
      "metadata": {
        "id": "eqMRJLbOYcwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    wrap_prompt(\"A 'flimple-dimple' is a magical and wonderful creature. An example of a sentence that uses the word 'flimple-dimple' is:\", \"user\"),\n",
        "    wrap_prompt(\"'Look at that marvelous flimple-dimple flying over there!'\", \"assistant\"),\n",
        "    wrap_prompt(\"To 'chillain' is to flit about rapidly in excitement. An example of a sentence that uses the words 'flimple-dimple' and 'chillain' is:\", \"user\")\n",
        "]\n",
        "\n",
        "m_print(get_response(list_of_prompts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "9f8bc99e-9c23-43b7-c7c6-b454d7329394"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\"The flimple-dimple was chillain from flower to flower, spreading joy with its magical presence in the garden.\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, leveraging the `assistant` role lets us teach our model new words, despite never needing to train the model.\n",
        "\n",
        "This is a (simple) example of \"In-Context Learning\"!"
      ],
      "metadata": {
        "id": "W0zn9-X2d23Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning New Tasks\n",
        "\n",
        "Now, we're not limited to just teaching our model silly words.\n",
        "\n",
        "Let's see that in action!"
      ],
      "metadata": {
        "id": "w26xNjO7WWQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    wrap_prompt(\"You provide appropriate sentiments for user's inputs and output them in a JSON object.\", \"user\"),\n",
        "    wrap_prompt(\"Man, I hate cheese!\", \"user\"),\n",
        "    wrap_prompt(\"{'sentiment' : 'negative'}\", \"assistant\"),\n",
        "    wrap_prompt(\"Oh boy, cheese is my favourite!\", \"user\"),\n",
        "    wrap_prompt(\"{'sentiment' : 'positive'}\", \"assistant\"),\n",
        "    wrap_prompt(\"Cheese is hella mid.\", \"user\"),\n",
        "    wrap_prompt(\"{'sentiment' : 'neutral'}\", \"assistant\"),\n",
        "    wrap_prompt(\"The Incredible Hulk is the best, and greatest superhero ever!\", \"user\"),\n",
        "]\n",
        "\n",
        "m_print(get_response(list_of_prompts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "an4OR-uyWp61",
        "outputId": "dbd85142-13f8-4df0-c255-3e76787ee12b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "{'sentiment': 'positive'}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how we're able to both instruct our model on the new task - as well as the output format all through prompting!"
      ],
      "metadata": {
        "id": "iGbR9Pr_XGKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment Portion\n",
        "\n",
        "Use few-shot prompting to allow the model to achieve a task it's not trained to do!"
      ],
      "metadata": {
        "id": "QLZzABmNQFa0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EK2xbXFZQMuS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "open_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}