<p align = "center" draggable=â€falseâ€ ><img src="https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719" 
     width="200px"
     height="auto"/>
</p>


<h1 align="center" id="heading">:wave: Welcome to LLM Engineering - The Foundations!</h1>

Welcome to the beginning of your journey to becoming an LLM Engineer! ğŸ‰ Excited to have you on your way to becoming an LLME practitioner! 

## What is LLM Ebngineering?

Data science and engineering teams around the world are being asked to training or fine-tune proprietary models. Everyone wants to build "ChatGPT for their data" and for their customers. To accomplish this, you need to know how OpenAI's GPT models were actually built, step-by-step.

This course will provide you with the foundational concepts and code you need to get started training, fine-tuning, and aligning your own LLMs.

From there, it's up to you to make the business case, organize the data, and secure the compute to give your company and your career a competitive LLM advantage.

## ğŸ“š About

This GitHub repository is your gateway to mastering the art of training, fine-tuning, and aligning LLM models like a pro. All assignments for the course will be released here for your building, shipping, and sharing adventures!

In this repository, we will dive deep into the best practices and technologies that empower LLM Engineers, to harness the full potential of LLMs. Here are just a few of the key methods and tools we'll explore:

#### LLME vs. MLE
- Best-practice tokenization strategies
- Understanding the role of embeddings
- Transformers = Attention Is All You Need
- Encoder/Decoder (BART-style)
- Encoder only (BERT-style)
- Decoder only (GPT-style)
- Build encoder-decoder and decoder-only models from scratch

#### LLM Training & Fine-Tuning
- GPT: "Semi-supervised sequence learning"
- GPT-2: "Zero-shot task transfer"
- GPT-3: "In-context learning"
- Understand datasets and benchmarks for training and tuning
- Compare model architecture, training, scale, limitations, and impact
- Implement unsupervised pretraining, supervised fine-tuning, and instruction-tuning

#### LLM Alignment
- InstructGPT: "Aligning LLMs with human intent"
- Reference (supervised fine-tuned), Policy (target), and Reward models
- Proximal policy optimization (PPO) algorithm
- Reinforcement Learning with Human Feedback (RLHF)
- Implement RLHF and RLAIF
- Limitations and broader impacts

## ğŸ™ Contributions

We believe in the power of collaboration. Contributions, ideas, and feedback are highly encouraged! Let's build the ultimate resource for LLMEs together. ğŸ¤

Feel free to reach out with any questions or suggestions. Happy coding! ğŸš€ğŸ”®

ğŸ‘¤ Follow us on [Twitter](https://twitter.com/AIMakerspace) and [LinkedIn](https://www.linkedin.com/company/ai-maker-space) for the latest news!

# Repo

[AI Makerspace](https://github.com/AI-Maker-Space/LLM-Engineering-The-Foundations-Cohort-1)
